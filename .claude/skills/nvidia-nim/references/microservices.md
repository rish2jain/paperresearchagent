# Nvidia-Nim - Microservices

**Pages:** 1

---

## NVIDIA NIM for Developers

**URL:** https://developer.nvidia.com/nim

**Contents:**
- NVIDIA NIM for Developers
- How It Works
  - Introductory Blog
  - Documentation
  - Introductory Video
  - Deployment Guide
- Build With NVIDIA NIM
  - Optimized Model Performance
  - Run AI Models Anywhere
  - Choose Among Thousands of AI Models and Customizations

NVIDIA NIM™ provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and RTX™ AI PCs and workstations. NIM microservices expose industry-standard APIs for simple integration into AI applications, development frameworks, and workflows and optimize response latency and throughput for each combination of foundation model and GPU.

Try APIsGet Started With NIM

NVIDIA NIM simplifies the journey from experimentation to deploying enterprise AI applications by providing enthusiasts, developers, and AI builders with pre-optimized models and industry-standard APIs for building powerful AI agents, co-pilots, chatbots, and assistants. With inference engines built on leading frameworks from NVIDIA and the community, including TensorRT, TensorRT-LLM, vLLM, SGLang, and more, NIM is engineered to facilitate seamless AI inferencing for the latest AI foundation models on NVIDIA GPUs.

Learn about NIM architecture, key features, and components.

Access guides, reference information, and release notes for running NIM on your infrastructure.

Learn how to deploy NIM on your infrastructure using a single command.

Get step-by-step instructions for self-hosting NIM on any NVIDIA accelerated infrastructure.

Improve AI application performance and efficiency with accelerated engines from NVIDIA and the community, including TensorRT, TensorRT-LLM, vLLM, SGLang, and more—prebuilt and optimized for low-latency, high-throughput inferencing on specific NVIDIA GPU systems.

Maintain security and control of applications and data with prebuilt microservices that can be deployed on NVIDIA GPUs anywhere—from RTX AI PCs, workstations, data centers, or the cloud. Download NIM inference microservices for self-hosted deployment, or take advantage of dedicated endpoints on Hugging Face to spin up instances in your preferred cloud.

Deploy a broad range of LLMs supported by vLLM, SGLang, or TensorRT-LLM, including community fine-tuned models and models fine-tuned on your data.

Get detailed observability metrics for dashboarding, and access Helm charts and guides for scaling NIM on Kubernetes.

Get started building AI applications powered by NIM using NVIDIA-hosted NIM API endpoints and generative AI examples from GitHub. See how easy it is to deploy retrieval-augmented generation (RAG) pipelines, agentic AI workflows, and more.

NVIDIA AI Blueprints are predefined, customizable AI workflows for creating and d

*[Content truncated]*

---
